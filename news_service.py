from datetime import date, timedelta
import re, urllib.parse, requests
from typing import List, Dict
from config import PROXIES, NEWSAPI_KEY, openai_client,GNEWS_API_KEY
from data import (AUTHORITATIVE_SOURCES, TOPIC_MAP,
                  CHARACTER_DESCRIPTION,ASIA_SUB_KEYS)
from utils import detect_lang


async def daily_digest(_):
    try:
        brief = await make_headline_brief("zh")
    except Exception as e:
        print("[DailyDigest Error]", e)
        return

    for cid in DAILY_DIGEST_TARGETS:
        try:
            await app.bot.send_message(chat_id=cid, text=brief[:4096])
        except Exception as e:
            print(f"[SendError â†’ {cid}]", e)

def fetch_newsapi_top_news(q: str = "technology OR politics", lang: str = "en", limit: int = 5) -> list[dict]:
    url = (
        f"https://newsapi.org/v2/top-headlines?"
        f"q={urllib.parse.quote_plus(q)}&"
        f"language={lang}&pageSize={limit}&apiKey={NEWSAPI_KEY}"
    )
    try:
        r = requests.get(url, proxies=PROXIES, timeout=10)
        r.raise_for_status()
        return r.json().get("articles", [])
    except Exception as e:
        print("[NewsAPI Fetch Error]", e)
        return []

def fetch_combined_news(
        keywords: List[str],
        *,
        max_articles: int = 30,
        lang: str = "en"
    ) -> List[Dict]:
    """ä¸€æ¬¡ OR æŸ¥è¯¢æ‹‰å…¨éƒ¨å…³é”®è¯æ–°é—»ï¼›è¿”å› â‰¤ max_articles ç¯‡æ–‡ç« """
    if not keywords:
        return []

    query = " OR ".join(f'"{k}"' for k in keywords)
    url   = (
        "https://gnews.io/api/v4/search?"
        f"q={urllib.parse.quote_plus(query)}&"
        f"token={GNEWS_API_KEY}&lang={lang}&max={max_articles}"
    )

    try:
        resp = requests.get(url, proxies=PROXIES, timeout=10)
        resp.raise_for_status()
        return resp.json().get("articles", [])
    except Exception as exc:
        print(f"[News FetchÂ Error] {exc}")
        return []


def fetch_authoritative_headlines(max_articles: int = 5) -> list[dict]:
    """
    åªä»ç™½åå•è‹±æ–‡æºæ‹‰ <max_articles> æ¡æœ€æ–°å¤´æ¡
    """
    url = (
        "https://newsapi.org/v2/top-headlines?"
        f"sources={','.join(AUTHORITATIVE_SOURCES)}"
        f"&pageSize={max_articles}"
        f"&apiKey={NEWSAPI_KEY}"
    )
    try:
        r = requests.get(url, proxies=PROXIES, timeout=10); r.raise_for_status()
        return r.json().get("articles", [])[:max_articles]
    except Exception as e:
        print("[NewsAPI Fetch Error]", e)
        return []


def fetch_general_news(*,
                       q: str,
                       from_date: str,
                       to_date: str,
                       max_articles: int,
                       lang: str = "en") -> list[dict]:
    url = (
        "https://newsapi.org/v2/everything?"
        f"q={urllib.parse.quote_plus(q)}"
        f"&from={from_date}&to={to_date}"
        f"&language={lang}"
        f"&sources={','.join(AUTHORITATIVE_SOURCES)}"
        f"&sortBy=publishedAt&pageSize={max_articles}"
        f"&apiKey={NEWSAPI_KEY}"
    )
    try:
        r = requests.get(url, proxies=PROXIES, timeout=10)
        r.raise_for_status()
        return r.json().get("articles", [])
    except Exception as exc:
        print("[NewsAPIÂ FetchÂ Error]", exc)
        return []

async def make_headline_brief(target_lang: str) -> str:
    """
    â‘  æ‹‰ 5 æ¡è‹±æ–‡å¤´æ¡
    â‘¡ GPT ç¿»è¯‘/æ’­æŠ¥ï¼ˆå«ç¼–å·ï¼‰
    â‘¢ åœ¨æ¯æ¡æ ‡é¢˜å**å•ç‹¬èµ·ä¸€è¡Œ**é™„ä¸ŠåŸæ–‡é“¾æ¥
    â‘£ GPT è¯„è®ºï¼ˆè‹¥å«å—é™ä¸»é¢˜åˆ™è·³è¿‡è¯¥æ¡ï¼‰
    """
    arts = fetch_authoritative_headlines(max_articles=5)
    if not arts:
        return {
            "zh": "æš‚æ—¶æŠ“ä¸åˆ°æ–°é—» ğŸ¥²",
            "ja": "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ ğŸ¥²",
            "en": "No fresh news found ğŸ¥²",
        }[target_lang]

    # --- 1. çº¯è‹±æ–‡æ ‡é¢˜ä¸² -------------------------------------------------
    english_block = "\n".join(
        f"{i+1}. {a['title']} ({a['source']['name']})"
        for i, a in enumerate(arts)
    )

    # --- 2. ç¿»è¯‘ / æ’­æŠ¥ --------------------------------------------------
    if target_lang == "zh":
        reporter_prompt = (
            "è¯·æŠŠä¸‹åˆ—è‹±æ–‡æ–°é—»æ ‡é¢˜ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**å¹¶ä¿æŒåºå·ï¼š\n\n"
            + english_block
        )
        reporter_header = "ã€ğŸ“¢ ä»Šæ—¥å¤´æ¡ã€‘\n"
        comment_prompt = (
            "è¯·å¯¹ä¸Šè¿°æ–°é—»é€æ¡å¹½é»˜ç‚¹è¯„ï¼ˆâ‰¤150å­—ï¼‰ã€‚"
            "è‹¥æŸæ¡æ¶‰åŠå—é™ä¸»é¢˜ï¼Œ**è·³è¿‡è¯¥æ¡**ï¼Œ"
            "ä¸è¦è¾“å‡ºä»»ä½•å†…å®¹æˆ–åºå·ã€‚"
        )
    elif target_lang == "ja":
        reporter_prompt = (
            "ä»¥ä¸‹ã®è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€ç•ªå·ã‚’ä¿ã£ãŸã¾ã¾åˆ—æŒ™ã—ã¦ãã ã•ã„ï¼š\n\n"
            + english_block
        )
        reporter_header = "ã€ğŸ“¢ ä»Šé€±ã®ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã€‘\n"
        comment_prompt = (
            "ä¸Šè¨˜ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’äº¤ãˆãŸçŸ­ã„ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ150å­—ä»¥å†…ï¼‰ã‚’é€æ¡ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚"
            "æ€§çš„å†…å®¹ã‚’å«ã‚€è¦‹å‡ºã—ã€ã¾ãŸã¯ä»–ã®åˆ¶é™ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Œã°**ã‚¹ã‚­ãƒƒãƒ—**ã—ã¦ãã ã•ã„ã€‚"
        )
    else:  # English
        reporter_prompt = english_block
        reporter_header = "ã€ğŸ“¢ Headlinesã€‘\n"
        comment_prompt = (
            "Give short playful comments (â‰¤150 words) for each headline. "
            "If a headline involves sexual content, explicit pornography, "
            "or other restricted topics, **skip that item**."
        )

    # news reports
    rep = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": reporter_prompt}],
        temperature=0,
    ).choices[0].message.content.strip()

    # --- 2â€‘2. insert links------------------------------------
    rep_lines = rep.splitlines()
    augmented = []
    for line in rep_lines:
        augmented.append(line)
        m = re.match(r"\s*(\d+)\.", line)
        if m:
            idx = int(m.group(1)) - 1
            if 0 <= idx < len(arts):
                url = arts[idx].get("url") or ""
                if url:
                    augmented.append(url)          # å•ç‹¬èµ·ä¸€è¡Œ
    rep_with_links = "\n".join(augmented)

    # --- GPT comments -------------------------------
    resp = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": CHARACTER_DESCRIPTION},
            {
                "role": "user",
                "content": rep_with_links + "\n\n" + comment_prompt,
            },
        ],
    ).choices[0].message.content.strip()

    # --- 4. final ---------------------------------------------
    return reporter_header + rep_with_links + "\n\nã€comments:ã€‘\n" + resp

async def make_topic_headline_brief(
    topic_key: str,
    period: str,
    target_lang: str,
    max_articles: int = 5
) -> str:
    """
    period: "day" | "week" | "month"
    """
    # -------- æ˜ å°„ --------
    query_en, display_zh, display_en = TOPIC_MAP[topic_key]

    # -------- è®¡ç®—æ—¶é—´çª—å£ --------
    today = date.today()
    if period == "day":
        frm, to = today, today
    elif period == "week":
        frm = today - timedelta(days=today.weekday())  # æœ¬å‘¨ä¸€
        to  = today
    else:  # "month"
        frm = today.replace(day=1)
        to  = today
    from_iso, to_iso = frm.isoformat(), to.isoformat()

    # -------- æŠ“æ–°é—» --------
    if query_en == "__ASIA_BUCKET__":
        raw_articles = []
        for sub_key in ASIA_SUB_KEYS:
            sub_query = TOPIC_MAP[sub_key][0]
            raw_articles.extend(  # â† æ²¡æœ‰ await
                fetch_general_news(
                    q=sub_query,
                    from_date=from_iso,
                    to_date=to_iso,
                    max_articles=max_articles,
                    lang="en"
                )
            )
        arts = deduplicate(raw_articles)[:max_articles]
    else:
        arts = fetch_general_news(
        q=query_en,
        from_date=from_iso,
        to_date=to_iso,
        max_articles=max_articles,
        lang="en")

    # -------- æ— æ–°é—»å…œåº• --------
    if not arts:
        if target_lang == "zh":
            return f"{'ä»Šæ—¥' if period=='day' else 'æœ¬å‘¨' if period=='week' else 'æœ¬æœˆ'}æ²¡æœ‰é‡å¤§{display_zh}æ–°é—» ğŸ¥²"
        elif target_lang == "ja":
            return f"{'æœ¬æ—¥' if period=='day' else 'ä»Šé€±' if period=='week' else 'ä»Šæœˆ'}ã®é‡è¦ãª{display_en}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ ğŸ¥²"
        else:
            return f"No major {display_en} news for this {period} ğŸ¥²"

    # -------- 1. è‹±æ–‡æ ‡é¢˜å— --------
    english_block = "\n".join(
        f"{i+1}. {a['title']} ({a['source']['name']})"
        for i, a in enumerate(arts)
    )

    # -------- 2. ç¿»è¯‘ / æ ‡é¢˜æ’­æŠ¥ --------
    if target_lang == "zh":
        reporter_prompt = "æŠŠä¸‹åˆ—è‹±æ–‡æ–°é—»æ ‡é¢˜ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**å¹¶ä¿æŒåºå·ï¼š\n\n" + english_block
        reporter_header = f"ã€ğŸ“¢ { 'ä»Šæ—¥' if period=='day' else 'æœ¬å‘¨' if period=='week' else 'æœ¬æœˆ' }{display_zh}å¤´æ¡ã€‘\n"
        comment_prompt  = "è¯·é€æ¡å¹½é»˜ç‚¹è¯„ï¼ˆâ‰¤150å­—ï¼‰ï¼Œè‹¥å«å—é™å†…å®¹è¯·è·³è¿‡ã€‚"
    elif target_lang == "ja":
        reporter_prompt = "ä»¥ä¸‹ã®è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€ç•ªå·ã‚’ä¿ã£ãŸã¾ã¾åˆ—æŒ™ã—ã¦ãã ã•ã„ï¼š\n\n" + english_block
        reporter_header = f"ã€ğŸ“¢ { 'æœ¬æ—¥' if period=='day' else 'ä»Šé€±' if period=='week' else 'ä»Šæœˆ' }ã®{display_zh}ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã€‘\n"
        comment_prompt  = "ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’äº¤ãˆçŸ­ãé€æ¡ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ150å­—ä»¥å†…ï¼‰ã€åˆ¶é™ãƒˆãƒ”ãƒƒã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚"
    else:
        reporter_prompt = english_block
        reporter_header = f"ã€ğŸ“¢ {display_en} Headlinesã€‘\n"
        comment_prompt  = ("Give playful comments (â‰¤150 words) for each headline. "
                           "Skip any item that involves restricted topics.")

    # 2-1 ç¿»è¯‘ / æ’­æŠ¥
    rep = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": reporter_prompt}],
        temperature=0
    ).choices[0].message.content.strip()

    # -------- 3. åŠ é“¾æ¥ --------
    out = []
    for line in rep.splitlines():
        out.append(line)
        m = re.match(r"\s*(\d+)\.", line)
        if m:
            idx = int(m.group(1)) - 1
            if 0 <= idx < len(arts):
                url = arts[idx].get("url") or ""
                if url:
                    out.append(url)
    rep_with_links = "\n".join(out)

    # -------- 4. è¯„è®º --------
    resp = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": CHARACTER_DESCRIPTION},
            {"role": "user",   "content": rep_with_links + "\n\n" + comment_prompt}
        ]
    ).choices[0].message.content.strip()

    if target_lang == "zh":
        return reporter_header + rep_with_links + "\n\nã€è¯„ã€‘\n" + resp
    else:
        return reporter_header + rep_with_links + "\n\nã€Commentsã€‘\n" + resp
